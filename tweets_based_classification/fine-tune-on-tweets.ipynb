{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/alon/')\n",
    "\n",
    "from hf_token import HF_AUTH_TOKEN \n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "BASE_DIR = Path(\"/home\", \"alon\", \"sdg\", \"tests\")\n",
    "CDLM_DIR = Path(BASE_DIR, \"cdlm\")\n",
    "DATA_FNAME = CDLM_DIR / \"tweets_datasets_of_eq_size\"\n",
    "TRAIN_PATH, TEST_PATH = DATA_FNAME / \"train_set.csv\", DATA_FNAME / \"test_set.csv\"\n",
    "MAX_TWEETS_NUM = 100\n",
    "DOC_SEP_START, DOC_SEP_END = r\"<doc-s>\", r\"</doc-s>\"\n",
    "PADDING_TOKEN = \"[PAD]\"\n",
    "LABELS_TO_5_PS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398e295",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f467845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, )\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61116bdd",
   "metadata": {},
   "source": [
    "## 1. Prepare Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770ce44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_to_5ps(sdg_label_int: int) -> int:\n",
    "    if sdg_label_int == 0:\n",
    "        return sdg_label_int\n",
    "    elif 1<=sdg_label_int<=5:\n",
    "        return 1\n",
    "    elif sdg_label_int in [6, 12, 13, 14, 15]:\n",
    "        return 2\n",
    "    elif 7<=sdg_label_int<=11:\n",
    "        return 3\n",
    "    elif sdg_label_int == 16:\n",
    "        return 4\n",
    "    elif sdg_label_int == 17:\n",
    "        return 5\n",
    "    else:\n",
    "        raise ValueError(\"Must provide a valid sdg label (int 0-17)\")\n",
    "        \n",
    "\n",
    "def cut_tweets_by_cutoff(concatenated_tweets: str) -> str:\n",
    "    concatenated_tweets = concatenated_tweets.rstrip(DOC_SEP_END)\n",
    "    tweets = concatenated_tweets.split(f\"{DOC_SEP_END}{DOC_SEP_START}\")[:CUR_TWEETS_NUM]\n",
    "    concatenated_tweets = f\"{DOC_SEP_END}{DOC_SEP_START}\".join(tweets) + DOC_SEP_END \n",
    "    return concatenated_tweets\n",
    "\n",
    "def get_tweets_based_df(csv_full_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_full_path, usecols=['label', 'concatenated_100_tweets'])\n",
    "    df['concatenated_tweets'] = df['concatenated_100_tweets'].apply(cut_tweets_by_cutoff)\n",
    "    df.drop(columns='concatenated_100_tweets', axis=1, inplace=True)\n",
    "    df.rename(columns={\"concatenated_tweets\": \"text\"}, inplace=True)       \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_description_based_df(csv_full_path: Path) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function returns a dataframe with 2 columns ('text', 'label') where text is each company's description\n",
    "    '''\n",
    "    df = pd.read_csv(csv_full_path, usecols=['label', 'description'])\n",
    "    df.rename(columns={\"description\": \"text\"}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_dataset(csv_full_path: Path, description: bool = True):\n",
    "    res = []\n",
    "    if description:\n",
    "        df = get_description_based_df(csv_full_path)\n",
    "        \n",
    "    else:\n",
    "        df = get_tweets_based_df(csv_full_path)\n",
    "        \n",
    "    if LABELS_TO_5_PS:\n",
    "        df['5ps_label'] = df['label'].apply(map_to_5ps)\n",
    "        df.drop(columns='label', axis=1, inplace=True)\n",
    "        df.rename(columns={\"5ps_label\": \"label\"}, inplace=True)\n",
    "        print('set(df[\"label\"])', set(df[\"label\"]))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        res.append({\"text\": row[\"text\"], \"label\": row[\"label\"]})\n",
    "        \n",
    "    return res\n",
    "\n",
    "def get_sorted_label_dict(ds) -> Dict[int, int]:\n",
    "    return sorted(dict(Counter([i[\"label\"] for i in ds])).items())\n",
    "\n",
    "\n",
    "def pairs_list_to_histogram(pl: Counter, title: str):\n",
    "    df = pd.DataFrame(pl, columns=['label', 'frequency'])\n",
    "    df.plot(kind='bar', x='label', title=title)\n",
    "\n",
    "\n",
    "CUR_TWEETS_NUM = MAX_TWEETS_NUM-75\n",
    "train_dataset, test_dataset = get_dataset(TRAIN_PATH), get_dataset(TEST_PATH)\n",
    "train_label_dist, test_label_dist = [get_sorted_label_dict(ds) for ds in [train_dataset, test_dataset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89556183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "eval_dataset = Dataset.from_list(test_dataset)\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'eval': eval_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5cf13",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_AUTH_TOKEN)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels= 6 if LABELS_TO_5_PS else 18, token=HF_AUTH_TOKEN)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7414e8",
   "metadata": {},
   "source": [
    "## 3. Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512) \n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a706a3",
   "metadata": {},
   "source": [
    "## 4. Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./alonm_tst_finetune5\", \n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_strategy=\"best\", \n",
    "                                  learning_rate=2e-5, \n",
    "                                  per_device_train_batch_size=1, \n",
    "                                  per_device_eval_batch_size=1, \n",
    "                                  num_train_epochs=15,\n",
    "                                  weight_decay=0.01,\n",
    "                                  save_total_limit=2,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"eval_loss\",\n",
    "                                  push_to_hub=False, \n",
    "                                  fp16=True,\n",
    "                                  gradient_checkpointing=True,\n",
    "                                 )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7920a75",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "plots_dir = Path(BASE_DIR, \"llm\", \"alonm_tst_finetune5\", f\"plots_tweets_{CUR_TWEETS_NUM}\")\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "EPOCH_NUM=1\n",
    "\n",
    "def plot_distribution(ax, hist_data, total_samples, title_prefix):\n",
    "    ax.bar(range(6 if LABELS_TO_5_PS else 18), hist_data, width=0.8, alpha=0.7)\n",
    "    ax.set_xticks(range(6 if LABELS_TO_5_PS else 18))\n",
    "    ax.set_xlabel('Label ID')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{title_prefix} Label Distribution (n={total_samples})')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    bins = range(7 if LABELS_TO_5_PS else 19) \n",
    "    ref_hist, _ = np.histogram(labels, bins=bins)\n",
    "    ref_total = len(labels)\n",
    "    \n",
    "    pred_hist, _ = np.histogram(predictions, bins=bins)\n",
    "    pred_total = len(predictions)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    plot_distribution(ax1, ref_hist, ref_total, \"Reference\")\n",
    "    plot_distribution(ax2, pred_hist, pred_total, \"Predicted\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    global EPOCH_NUM\n",
    "    plt.savefig(plots_dir / f\"distribution_comparison_{EPOCH_NUM}.png\")\n",
    "    plt.close()\n",
    "    EPOCH_NUM+=1\n",
    "    \n",
    "    ref_dist = ref_hist / ref_hist.sum()\n",
    "    pred_dist = pred_hist / pred_hist.sum()\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    ref_dist_smooth = ref_dist + epsilon\n",
    "    ref_dist_smooth = ref_dist_smooth / ref_dist_smooth.sum()\n",
    "    pred_dist_smooth = pred_dist + epsilon\n",
    "    pred_dist_smooth = pred_dist_smooth / pred_dist_smooth.sum()\n",
    "    \n",
    "    kl_div = entropy(ref_dist_smooth, pred_dist_smooth)\n",
    "    \n",
    "    m_dist = 0.5 * (ref_dist_smooth + pred_dist_smooth)\n",
    "    js_div = 0.5 * (entropy(ref_dist_smooth, m_dist) + entropy(pred_dist_smooth, m_dist))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(ref_dist, pred_dist))\n",
    "    \n",
    "    return {f\"f1_{avg_type}\": f1_score(labels, predictions, average=avg_type) for avg_type in ['weighted', 'macro', 'micro']} | {\"kl_div\": kl_div, \"js_div\":js_div, \"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43760d9d",
   "metadata": {},
   "source": [
    "## 6. Create Trainer and Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_datasets[\"train\"],\n",
    "                  eval_dataset=tokenized_datasets[\"eval\"],\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  )\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(\n",
    "    enable_flash=True,  \n",
    "    enable_mem_efficient=True,  \n",
    "    enable_math=False   \n",
    "):   \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda50499",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b80f8e-0323-4fbe-bff3-6ed15a9314a1",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

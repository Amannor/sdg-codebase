{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7457a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/alon/')\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_SDP_KERNEL\"] = \"math\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "BASE_DIR = Path(\"/home\", \"storage\", \"home\", \"alon\", \"sdg\", \"tests\")\n",
    "CDLM_DIR = Path(BASE_DIR, \"cdlm\")\n",
    "DATA_FNAME = CDLM_DIR / \"tweets_datasets_of_eq_size\"\n",
    "TRAIN_PATH, TEST_PATH = DATA_FNAME / \"train_set.csv\", DATA_FNAME / \"test_set.csv\"\n",
    "MAX_TWEETS_NUM = 100\n",
    "DOC_SEP_START, DOC_SEP_END = r\"<doc-s>\", r\"</doc-s>\"\n",
    "PADDING_TOKEN = \"[PAD]\"\n",
    "LABELS_TO_5_PS = True #Toggle between True\\False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f0054",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b408b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, )\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a0422",
   "metadata": {},
   "source": [
    "## 1. Prepare Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6c1bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(df[\"label\"]) {0, 1, 2, 3, 4, 5}\n",
      "set(df[\"label\"]) {0, 1, 2, 3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "def map_to_5ps(sdg_label_int: int) -> int:\n",
    "    if sdg_label_int == 0:\n",
    "        return sdg_label_int\n",
    "    elif 1<=sdg_label_int<=5:\n",
    "        return 1\n",
    "    elif sdg_label_int in [6, 12, 13, 14, 15]:\n",
    "        return 2\n",
    "    elif 7<=sdg_label_int<=11:\n",
    "        return 3\n",
    "    elif sdg_label_int == 16:\n",
    "        return 4\n",
    "    elif sdg_label_int == 17:\n",
    "        return 5\n",
    "    else:\n",
    "        raise ValueError(\"Must provide a valid sdg label (int 0-17)\")\n",
    "        \n",
    "\n",
    "def cut_tweets_by_cutoff(concatenated_tweets: str) -> str:\n",
    "    concatenated_tweets = concatenated_tweets.rstrip(DOC_SEP_END)\n",
    "    tweets = concatenated_tweets.split(f\"{DOC_SEP_END}{DOC_SEP_START}\")[:CUR_TWEETS_NUM]\n",
    "    concatenated_tweets = f\"{DOC_SEP_END}{DOC_SEP_START}\".join(tweets) + DOC_SEP_END \n",
    "    return concatenated_tweets\n",
    "\n",
    "def get_tweets_based_df(csv_full_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_full_path, usecols=['label', 'concatenated_100_tweets'])\n",
    "    df['concatenated_tweets'] = df['concatenated_100_tweets'].apply(cut_tweets_by_cutoff)\n",
    "    df.drop(columns='concatenated_100_tweets', axis=1, inplace=True)\n",
    "    df.rename(columns={\"concatenated_tweets\": \"text\"}, inplace=True)       \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_description_based_df(csv_full_path: Path) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function returns a dataframe with 2 columns ('text', 'label') where text is each company's description\n",
    "    '''\n",
    "    df = pd.read_csv(csv_full_path, usecols=['label', 'description'])\n",
    "    df.rename(columns={\"description\": \"text\"}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_dataset(csv_full_path: Path, description: bool = True):\n",
    "    res = []\n",
    "    if description:\n",
    "        df = get_description_based_df(csv_full_path)\n",
    "        \n",
    "    else:\n",
    "        df = get_tweets_based_df(csv_full_path)\n",
    "        \n",
    "    if LABELS_TO_5_PS:\n",
    "        df['5ps_label'] = df['label'].apply(map_to_5ps)\n",
    "        df.drop(columns='label', axis=1, inplace=True)\n",
    "        df.rename(columns={\"5ps_label\": \"label\"}, inplace=True)\n",
    "        print('set(df[\"label\"])', set(df[\"label\"]))\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        res.append({\"text\": row[\"text\"], \"label\": row[\"label\"]})\n",
    "        \n",
    "    return res\n",
    "\n",
    "def get_sorted_label_dict(ds) -> Dict[int, int]:\n",
    "    return sorted(dict(Counter([i[\"label\"] for i in ds])).items())\n",
    "\n",
    "\n",
    "def pairs_list_to_histogram(pl: Counter, title: str):\n",
    "    df = pd.DataFrame(pl, columns=['label', 'frequency'])\n",
    "    df.plot(kind='bar', x='label', title=title)\n",
    "\n",
    "\n",
    "CUR_TWEETS_NUM = MAX_TWEETS_NUM #Values tested: MAX_TWEETS_NUM, MAX_TWEETS_NUM-25, MAX_TWEETS_NUM-50, MAX_TWEETS_NUM-75\n",
    "train_dataset, test_dataset = get_dataset(TRAIN_PATH), get_dataset(TEST_PATH)\n",
    "train_label_dist, test_label_dist = [get_sorted_label_dict(ds) for ds in [train_dataset, test_dataset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912598d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n",
      "254\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "eval_dataset = Dataset.from_list(test_dataset)\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'eval': eval_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662af82",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "971e1cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Toggle between bert-base-uncased and meta-llama/Llama-3.2-1B-Instruct. Note original paper (Bar 2022) also checked bert-large-uncased but found it more cumbersome without significant improve in results\n",
    "model_name = \"bert-base-uncased\"\n",
    "if model_name == \"meta-llama/Llama-3.2-1B-Instruct\":\n",
    "    from hf_token import HF_AUTH_TOKEN \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_AUTH_TOKEN)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels= 6 if LABELS_TO_5_PS else 18, token=HF_AUTH_TOKEN)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels= 6 if LABELS_TO_5_PS else 18)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f9aef",
   "metadata": {},
   "source": [
    "## 3. Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c33c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2368ee584a24d12ae9a70c72a807afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7f44be8e234357a0e2d8f82d920519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512) \n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7903a",
   "metadata": {},
   "source": [
    "## 4. Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702d55cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/new_env_alon/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 if model_name == \"meta-llama/Llama-3.2-1B-Instruct\" else 16\n",
    "training_args = TrainingArguments(output_dir=\"./alonm_tst_finetune\", \n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_strategy=\"steps\",#Consider \"best\" for the llm\n",
    "                                  learning_rate=2e-5, \n",
    "                                  per_device_train_batch_size=batch_size, \n",
    "                                  per_device_eval_batch_size=batch_size, \n",
    "                                  num_train_epochs=15,\n",
    "                                  weight_decay=0.01,\n",
    "                                  save_total_limit=2,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"eval_loss\",\n",
    "                                  push_to_hub=False, \n",
    "                                  fp16=True,\n",
    "                                  gradient_checkpointing=True\n",
    "                                 )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b941849",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581227c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "plots_dir = Path(BASE_DIR, \"llm\" if model_name == \"meta-llama/Llama-3.2-1B-Instruct\" else \"bert\", \"alonm_tst_finetune\", f\"plots_tweets_{CUR_TWEETS_NUM}\")\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "EPOCH_NUM=1\n",
    "\n",
    "def plot_distribution(ax, hist_data, total_samples, title_prefix):\n",
    "    ax.bar(range(6 if LABELS_TO_5_PS else 18), hist_data, width=0.8, alpha=0.7)\n",
    "    ax.set_xticks(range(6 if LABELS_TO_5_PS else 18))\n",
    "    ax.set_xlabel('Label ID')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{title_prefix} Label Distribution (n={total_samples})')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    bins = range(7 if LABELS_TO_5_PS else 19) \n",
    "    ref_hist, _ = np.histogram(labels, bins=bins)\n",
    "    ref_total = len(labels)\n",
    "    \n",
    "    pred_hist, _ = np.histogram(predictions, bins=bins)\n",
    "    pred_total = len(predictions)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    plot_distribution(ax1, ref_hist, ref_total, \"Reference\")\n",
    "    plot_distribution(ax2, pred_hist, pred_total, \"Predicted\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    global EPOCH_NUM\n",
    "    plt.savefig(plots_dir / f\"distribution_comparison_{EPOCH_NUM}.png\")\n",
    "    plt.close()\n",
    "    EPOCH_NUM+=1\n",
    "    \n",
    "    ref_dist = ref_hist / ref_hist.sum()\n",
    "    pred_dist = pred_hist / pred_hist.sum()\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    ref_dist_smooth = ref_dist + epsilon\n",
    "    ref_dist_smooth = ref_dist_smooth / ref_dist_smooth.sum()\n",
    "    pred_dist_smooth = pred_dist + epsilon\n",
    "    pred_dist_smooth = pred_dist_smooth / pred_dist_smooth.sum()\n",
    "    \n",
    "    kl_div = entropy(ref_dist_smooth, pred_dist_smooth)\n",
    "    \n",
    "    m_dist = 0.5 * (ref_dist_smooth + pred_dist_smooth)\n",
    "    js_div = 0.5 * (entropy(ref_dist_smooth, m_dist) + entropy(pred_dist_smooth, m_dist))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(ref_dist, pred_dist))\n",
    "    \n",
    "    return {f\"f1_{avg_type}\": f1_score(labels, predictions, average=avg_type) for avg_type in ['weighted', 'macro', 'micro']} | {\"kl_div\": kl_div, \"js_div\":js_div, \"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfafb77",
   "metadata": {},
   "source": [
    "## 6. Create Trainer and Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915ba782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/new_env_alon/lib/python3.9/contextlib.py:87: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [960/960 08:52, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Kl Div</th>\n",
       "      <th>Js Div</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>1.530436</td>\n",
       "      <td>0.662149</td>\n",
       "      <td>0.465365</td>\n",
       "      <td>0.665354</td>\n",
       "      <td>0.214821</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.012241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tokenized_datasets[\"train\"],\n",
    "                  eval_dataset=tokenized_datasets[\"eval\"],\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  )\n",
    "if model_name == \"meta-llama/Llama-3.2-1B-Instruct\":\n",
    "    with torch.backends.cuda.sdp_kernel(\n",
    "        enable_flash=True,  \n",
    "        enable_mem_efficient=True,  \n",
    "        enable_math=False   \n",
    "    ):   \n",
    "        trainer.train()\n",
    "else:\n",
    "    with torch.backends.cuda.sdp_kernel(\n",
    "        enable_flash=True,\n",
    "        enable_mem_efficient=True,\n",
    "        enable_math=True,  # fallback safety\n",
    "    ):\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8199b6f",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da67e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b28c39",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
